
# Data Visualization Through Grailed

## Background:

Recently, I've been buying and selling a lot of clothes on the website Grailed, which got me interested in the prices of brands that I'm interested in and how they've changed over the years. So, I decided to develop a program to capture, clean, and visualize this data into an easily interpretable way.

## The Process:

As this is a data collection tool, I started the program by opening a selenium driver in Chrome on Grailed's login page. Then, I created a function to login to Grailed, because in order to view sold listings, you have to be signed in. To do this, I had to implement a captcha bypass using OpenAI's Whisper program, which allowed me to transcribe and paste in the text (credits to @theriley106 for this). Then, I implemented two options: one being the brand the user would like to navigate too, and the other being if they would like to view either current or sold listings. This would allow for an easier sense for data collection, as the brand and item availability all have effects on the data, each one with their own unique numbers. Once the options were submitted, I could start developing the scroll. I used selenium to scroll from the top to the bottom of the page 'infinitely' (I say this with quotes because Grailed only lets drivers scroll for 1000 feed items). Once scrolling through all of the data, I used BeautifulSoup4 to capture each unique feature of each item, those being the titles, current prices, original prices, sizes, current listing dates, original listing dates, image links, and listing links. These were then converted to a Pandas DataFrame, from which I could then optimize and use to visualize and make conclusions about my data.

## The Problems/How I addressed Them:

As much as I would have liked to just stopped here, there was still a slew of issues present in the way that I was collecting my data. For one, Grailed has a massive issue with false positives, where users will list the brand of clothing as said brand, but it ends up being something different in the description. This is generally used to garner more views and hits in their algorithm, but it isn't very good for data collection, as it can drastically change our numbers. To fix this, I filtered out all rows in the data that didn't have the brand listed in the title. And while this may have removed some actual data, I believe that it did far better than worse, as I generally removed ~100-200 false positives per 1000 item scan. Fixing this helped, but I still had many issues to fix, such as needing to collect the categories of each clothing item for my database. Grailed's category system isn't very good, and if I tried to implement it, I wouldn't be able to generate a lot of data, so I had to categorize items after the parsing was complete. To do this, I manually created a dictionary consisting of each category: accessories, bottoms, dresses, outerwear, shoes, skirts, and tops, and their unique garments. This worked out for the most part, but what about the cases where the brand was listed, but not a category? To fix, I trained a deep neural network using TensorFlow and Keras to classify each image based on their categories. To generate this model, I first had to create an image database. My first attempt was to use the fashion_mnist's database and train a model based on that, but it didn't work, since each image was set with a black background and grayscale imaging. Even after preprocessing my data to match the dataset's it still didn't work, since none of my images were ever truly going to be able to match fashion_mnist's level of preprocessing. Next, I decided to use the dataset put out by @alexeygrigorev called 'clothing-dataset-small.' This dataset consisted of around 5000 images with 10 categories, which worked better, but not good enough. So to solve this, I generated my own dataset on top of this. I grabbed each image src via my image link array, and then downloaded each image into separate train and validation folders, each consisting of my 7 categories. I then implemented an ImageHash function to remove duplicates, which would help prevent overfitting in my data. Finally, my dataset was complete, and it was time to train my model.

## Training the First Model:

For my first model, I decided to use the pretrained 'imagenet' classifier models, since they were the best bang for my buck, given my dataset only had around ~6500 images to go off of. After many tests, I went with the Xception model, as its great object detection worked perfectly with my sometimes hard-to-read garments that were included in the dataset. From there, I implemented gradient descent via the adam optimizer, and used categorical crossentropy to work with my multiclass system. I then preproccessed the input of my train and validation images, and set their dimensions to 150 by 150, as this gave me the best results for the least amount of hassle in generating the model. I then was ready to generate the model, which I did with a learning rate of .001 to best optimize time for performance, as lower learning rates didn't yield any better results. I also trained it on 18 epochs, and used EarlyStopping after 2 epochs to prevent any overfitting. I then saved this model to my current directory, and stopped there. The train accuracy and loss was 92% and 30% respectively, and the validation accuracy and loss was 75% and 72% respectively. This high validation loss can be explained by a lack of diverse training data, which I will fix soon in the future by providing more images for it to be trained on. All problems aside, my data was complete, and it was finally time to plot it.

## Plotting the Data

To plot the data, I decided to go with Matplotlib, as its usability with Pandas is always easy and effective. To start, I chose to plot my data based on four categories: volume, price per category, price over time, and price by size. All of these showed unique features of my data which could be interpreted in their own ways. To plot by volume, I simply took the counts of each category, and plotted them on a bar graph, with the x-axis being the categories and the y-axis being the volume. For my next graph, I took all of the unique categories, and looped through each one. Then, I indexed into each unique category and took the mean of all of their original prices, which I then converted to a bar graph, with each bar on the x-axis being a different category and the y-axis representing the price. For my third graph, I decided to have multiple options, those being to graph by days, months, and years. This would give the user unique options to view the price at different intervals, so they could make accurate and knowledgeable predictions. I also provided the option to narrow the data to only show information for one category, instead of just for the entire brand. But before I could any plotting, I first had to get the actual dates of the items listed. Grailed doesn't do things by date, but rather, by intervals of time, such as minutes, hours, days, months, and years. They also provide keywords such as about, almost, over, and sold, which I promptly removed from the data. Then to convert the intervals to actual dates, I used the datetime and timedelta libraries to extract the proper dates, which would go into their own unique column titled 'Relative Date.' Then, I could finally start the plotting. First, I did the 'days' option, which started by creating a new 'Days' column. From there, I filtered the DataFrame to only include items from the last 31 days (1 month). Then, I calculated the average price and volume of each day, and then plotted it as a line graph, which I thought was appropriate since this was a day-by-day time graph. Next, I created my months graph, which was done by grouping all months together by combining their respective year and respective month. Then, I generated the volume of each months by taking the count of each month's items and normalizing it. Then, I implemented a bar graph to plot the data, and included a color gradient to represent the volume of each bar, with dark purple being the lowest and light yellow being the highest. Finally, I created the years graph by grouping all of the years into their own respective categories, and, after preprocessing, generating, and normalizing each year's volume, I plotted each year as a bar on a bar graph, complete with the same color gradient for volume. Finally, the last graph that I created was a size graph, which would take the group all of the sizes by their mean and count to generate volume, and then would take their price and print it as a bar graph in ascending order, from lowest price to highest price. I also included the same volume slider to see the most popular and least popular sizes.

## Problems With Plotting

For the most part, plotting these graphs went along great, but I did have a few issues. The first issue I came across had to do with trying to plot data that didn't exist. Grailed lists their sold listings in descending order from most recent sale to oldest sale,and in the event that all 1000 items I parsed through contained the 'days' keyword, that meant that there wouldn't be any option to plot months or years. This would cause errors when I tried to, so I implemented a catch to scan if there were only days featured in the data, and if that was true, then it would default to printing the days graph. Another issue that I came across was in my months graph, where there would accidentally be an artificially high volume generated at any month that was a year ago from the current date. This happened because once listings go over a year old, they display as being sold '1 year ago.' Effectively, this keyword removed that month-year feature that my graph was using, and would instead just group all listings under the month from a year ago. For example, when I was testing, the month June 2022, would appear with an incredibly high volume, since that month was the current month that was a year ago from when I was testing the program. To solve this, I simply checked to make sure that the date would only get fed into the graph if it was in between 11 months ago and today, which fixed my volume issue.

## Next Steps

After collecting, cleaning, and plotting my data, there was still one more thing that I wanted to do - make predictions about future data. To do this, I decided to train another model using PyTorch's LSTM module. To start, I first grouped the DataFrame by year and month, and then calculated the mean price and reset the index, which would aggregate the data to have average prices for each month. Then, I generated a volume column by the count of prices per month, and split the data into test and train sets. After that, I scaled the price and volume features using MinMaxScaler, and converted the scaled data into PyTorch tensors. Then, I defined the LSTM module as the LSTM class, the loss function as MSE, and the optimizer as Adam. Then I trained the LSTM model using the training data, and switched the model to evaluate future predictions. Then, I scaled the predicted prices back to their original counterparts using the inverse transform, and calculated the mean of their prices. Finally, I used the system's default locale to convert the mean price to currency, and returned the prediction.

## Final Thoughts

Overall, this project provided a useful insight into how fashion trends change over time. With the boom of social medias such as TikTok in the fashion community, I could actively see how certain echochambers and groups on social media contributed to the rising prices of certain brands on Grailed, which was amazing to see. 

## What Lies Ahead

In the future, I would like to convert this project to a streamlit app so it can be more user friendly. At the moment, I have a discord bot working which can be found at this link: https://github.com/jgurien03/grailed-bot 

If you have any concerns or questions about the project, you can email me at jgurien@ucsc.edu
