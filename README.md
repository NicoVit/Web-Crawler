
# Data Visualization Through Grailed

## Background:

Recently, I've been buying and selling a lot of clothes on the website Grailed, which got me interested in the prices of brands that I'm interested in and how they've changed over the years. So, I decided to develop a program to capture, clean, and visualize this data into an easily interpretable way.

## The Process:

As this is a data collection tool, I started the program by opening a selenium driver in Chrome on Grailed's login page. Then, I created a function to login to Grailed, because in order to view sold listings, you have to be signed in. To do this, I had to implement a captcha bypass using OpenAI's Whisper program, which allowed me to transcribe and paste in the text (credits to @theriley106 for this). Then, I implemented two options: one being the brand the user would like to navigate too, and the other being if they would like to view either current or sold listings. This would allow for an easier sense for data collection, as the brand and item availability all have effects on the data, each one with their own unique numbers. Once the options were submitted, I could start developing the scroll. I used selenium to scroll from the top to the bottom of the page 'infinitely' (I say this with quotes because Grailed only lets drivers scroll for 1000 feed items). Once scrolling through all of the data, I used BeautifulSoup4 to capture each unique feature of each item, those being the titles, current prices, original prices, sizes, current listing dates, original listing dates, image links, and listing links. These were then converted to a Pandas DataFrame, from which I could then optimize and use to visualize and make conclusions about my data.

## The Problems/How I addressed Them:

As much as I would have liked to just stopped here, there was still a slew of issues present in the way that I was collecting my data. For one, Grailed has a massive issue with false positives, where users will list the brand of clothing as said brand, but it ends up being something different in the description. This is generally used to garner more views and hits in their algorithm, but it isn't very good for data collection, as it can drastically change our numbers. To fix this, I filtered out all rows in the data that didn't have the brand listed in the title. And while this may have removed some actual data, I believe that it did far better than worse, as I generally removed ~100-200 false positives per 1000 item scan. Fixing this helped, but I still had many issues to fix, such as needing to collect the categories of each clothing item for my database. Grailed's category system isn't very good, and if I tried to implement it, I wouldn't be able to generate a lot of data, so I had to categorize items after the parsing was complete. To do this, I manually created a dictionary consisting of each category: accessories, bottoms, dresses, outerwear, shoes, skirts, and tops, and their unique garments. This worked out for the most part, but what about the cases where the brand was listed, but not a category? To fix, I trained a deep neural network using TensorFlow and Keras to classify each image based on their categories. To generate this model, I first had to create an image database. My first attempt was to use the fashion_mnist's database and train a model based on that, but it didn't work, since each image was set with a black background and grayscale imaging. Even after preprocessing my data to match the dataset's it still didn't work, since none of my images were ever truly going to be able to match fashion_mnist's level of preprocessing. Next, I decided to use the dataset put out by @alexeygrigorev called 'clothing-dataset-small.' This dataset consisted of around 5000 images with 10 categories, which worked better, but not good enough. So to solve this, I generated my own dataset on top of this. I grabbed each image src via my image link array, and then downloaded each image into separate train and validation folders, each consisting of my 7 categories. I then implemented an ImageHash function to remove duplicates, which would help prevent overfitting in my data. Finally, my dataset was complete, and it was time to train my model.

## Training the First Model:

For my first model, I decided to use the pretrained 'imagenet' classifier models, since they were the best bang for my buck, given my dataset only had around ~6500 images to go off of. After many tests, I went with the Xception model, as its great object detection worked perfectly with my sometimes hard-to-read garments that were included in the dataset. From there, I implemented gradient descent via the adam optimizer, and used categorical crossentropy to work with my multiclass system. I then preproccessed the input of my train and validation images, and set their dimensions to 150 by 150, as this gave me the best results for the least amount of hassle in generating the model. I then was ready to generate the model, which I did with a learning rate of .001 to best optimize time for performance, as lower learning rates didn't yield any better results. I also trained it on 18 epochs, and used EarlyStopping after 2 epochs to prevent any overfitting. I then saved this model to my current directory, and stopped there. The train accuracy and loss was 92% and 30% respectively, and the validation accuracy and loss was 75% and 72% respectively. This high validation loss can be explained by a lack of diverse training data, which I will fix soon in the future by providing more images for it to be trained on. 

## Plotting the Data

To plot the data, I decided to go with Matplotlib, as its usability with Pandas is always easy and effective. To start, I chose to plot my data based on four categories: volume, price per category, price over time, and price by size. All of these showed unique features of my data which could be interpreted in their own ways. To plot by volume, I simply took the counts of each category, and plotted them on a bar graph, with the x-axis being the categories and the y-axis being the volume. For my next graph, I took all of the unique categories, and looped through each one. Then, I indexed into each unique category and took the mean of all of their original prices, which I then converted to a bar graph, with each bar on the x-axis being a different category and the y-axis representing the price. For my third graph, I decided to have multiple options, those being to graph by days, months, and years. This would give the user unique options to view the price at different intervals, so they could make accurate and knowledgeable predictions. I also provided the option to narrow the data to only show information for one category, instead of just for the entire brand. But before I could any plotting, I first had to get the actual dates of the items listed. Grailed doesn't do things by date, but rather, by intervals of time, such as minutes, hours, days, months, and years. They also provide keywords such as about, almost, over, and sold, which I promptly removed from the data. Then to convert the intervals to actual dates, I used the datetime and timedelta libraries to extract the proper dates, which would go into their own unique column titled 'Relative Date.' Then, I could finally start the plotting. First, I did the 'days' option, which would 
